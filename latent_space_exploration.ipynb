{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "latent_space_exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/BUGAN/blob/master/latent_space_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwbGaO5aJS8y"
      },
      "source": [
        "Before starting please save the notebook in your drive by clicking on `File -> Save a copy in drive`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQtEpYYRh9LM",
        "cellView": "form"
      },
      "source": [
        "#@markdown Mount google drive.\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if we have linked the folder\n",
        "from pathlib import Path\n",
        "if not Path(\"/content/drive/My Drive/IRCMS_GAN_collaborative_database\").exists():\n",
        "    print(\n",
        "        \"Shortcut to our shared drive folder doesn't exits.\\n\\n\"\n",
        "        \"\\t1. Go to the google drive web UI\\n\"\n",
        "        \"\\t2. Right click shared folder IRCMS_GAN_collaborative_database and click \\\"Add shortcut to Drive\\\"\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b-Vx_QVHLXH"
      },
      "source": [
        "#@title Configure dataset\n",
        "#@markdown - choose whether to use wandb id or use checkpoint_path to select checkpoint file\n",
        "\n",
        "#@markdown Choice 1: wandb id and project_name to select checkpoint file\n",
        "#@markdown - set `\"run_id\"` to resume a run (for example: `u9imsvva`)\n",
        "#@markdown - The id of the current run is shown below in the cell with `wandb.init()` or you can find it in the wandb web UI.\n",
        "id = \"2hg5kfx4\" #@param {type:\"string\"}\n",
        "#@markdown - Enter project name (either `handtool-gan` or `tree-gan`)\n",
        "project_name = \"tree-gan\" #@param [\"tree-gan\", \"handtool-gan\"]\n",
        "\n",
        "#@markdown Choice 2: file path and model type to select saved checkpoint .ckpt file\n",
        "#@markdown - For example via the file browser on the left to locate and right click to copy the path.\n",
        "#@markdown - file path example: `/content/drive/My Drive/h/k/checkpoint.ckpt` \n",
        "ckpt_file_location = \"\" #@param {type:\"string\"}\n",
        "#@markdown - Enter trained neural network model type\n",
        "#@markdown - (may be necessary for wandb_id if selected_model is not saved in config)\n",
        "selected_model = \"VAE\"    #@param [\"VAEGAN\", \"GAN\", \"VAE\"]\n",
        "\n",
        "#@markdown For VAEGAN/VAE only: Enter 2 mesh file location to specify 2 latent vectors.   \n",
        "meshfile1 = \"/content/drive/My Drive/IRCMS_GAN_collaborative_database/Research/Peter/Tree_3D_models_obj/obj_files/old_1.obj\" #@param {type:\"string\"}\n",
        "meshfile2 = \"/content/drive/My Drive/IRCMS_GAN_collaborative_database/Research/Peter/Tree_3D_models_obj/obj_files/maple_example2.obj\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter how many samples to generate\n",
        "test_num_samples = 20    #@param {type:\"integer\"}\n",
        "#@markdown Enter export location (folder/directory).   \n",
        "export_location = f\"/content/drive/My Drive/IRCMS_GAN_collaborative_database/Experiments/exportObjects/{id}\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "if id and ckpt_file_location:\n",
        "    raise Exception(\"Only one of id / ckpt_file_location can be set!\")\n",
        "if (not id) and (not ckpt_file_location):\n",
        "    raise Exception(\"Please set id / ckpt_file_location!\")\n",
        "\n",
        "if id:\n",
        "    print(\"id:\", id)\n",
        "    print(\"project_name:\", project_name)\n",
        "else:\n",
        "    print(\"selected_model:\", selected_model)\n",
        "    print(\"ckpt_file_location:\", ckpt_file_location)\n",
        "print(\"test_num_samples:\", test_num_samples)\n",
        "print(\"export_location:\", export_location)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp1sqgZf5lRv"
      },
      "source": [
        "from argparse import Namespace, ArgumentParser\n",
        "#@markdown Install wandb and log in\n",
        "entity=\"bugan\"\n",
        "rev_number = None\n",
        "if id:\n",
        "    !pip install wandb\n",
        "    output.clear()\n",
        "    #find wandb API key file to auto login\n",
        "    import wandb\n",
        "    wandb_drive_netrc_path = Path(\"drive/My Drive/colab/.netrc\")\n",
        "    wandb_local_netrc_path = Path(\"/root/.netrc\")\n",
        "    if wandb_drive_netrc_path.exists():\n",
        "        import shutil\n",
        "\n",
        "        print(\"Wandb .netrc file found, will use that to log in.\")\n",
        "        shutil.copy(wandb_drive_netrc_path, wandb_local_netrc_path)\n",
        "    else:\n",
        "        print(\n",
        "            f\"Wandb config not found at {wandb_drive_netrc_path}.\\n\"\n",
        "            f\"Using manual login.\\n\\n\"\n",
        "            f\"To use auto login in the future, finish the manual login first and then run:\\n\\n\"\n",
        "            f\"\\t!mkdir -p '{wandb_drive_netrc_path.parent}'\\n\"\n",
        "            f\"\\t!cp {wandb_local_netrc_path} '{wandb_drive_netrc_path}'\\n\\n\"\n",
        "            f\"Then that file will be used to login next time.\\n\"\n",
        "        )\n",
        "\n",
        "    !wandb login\n",
        "\n",
        "    #read information (run config, etc) stored online\n",
        "        #all config will be replaced by the stored one in wandb\n",
        "    api = wandb.Api()\n",
        "    previous_run = api.run(f\"{entity}/{project_name}/{id}\")\n",
        "    config = Namespace(**previous_run.config)\n",
        "        #load selected_model, rev_number in the config\n",
        "    if hasattr(config, \"selected_model\"):\n",
        "        selected_model = config.selected_model\n",
        "    if hasattr(config, \"rev_number\"):\n",
        "        rev_number = config.rev_number\n",
        "\n",
        "    run = wandb.init(project=project_name, id=id, entity=entity, resume=True, dir=\"./\", group=selected_model)\n",
        "\n",
        "    output.clear()\n",
        "    print(\"run id: \" + str(wandb.run.id))\n",
        "    print(\"run name: \" + str(wandb.run.name))\n",
        "    print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUduMFlzmKmO"
      },
      "source": [
        "# To just train a model, no edits should be required in any cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3jIEKP7i4Nt"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "from pathlib import Path\n",
        "os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
        "\n",
        "%cd /content/drive/My Drive/IRCMS_GAN_collaborative_database/Experiments/\n",
        "\n",
        "run_path = \"./\"\n",
        "\n",
        "!apt-get update\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "!pip install trimesh\n",
        "!apt install -y xvfb\n",
        "!pip install trimesh xvfbwrapper\n",
        "#bugan package\n",
        "if rev_number is not None:\n",
        "    %pip install --upgrade git+https://github.com/buganart/BUGAN.git@{rev_number}#egg=bugan\n",
        "else:\n",
        "    %pip install --upgrade git+https://github.com/buganart/BUGAN.git#egg=bugan\n",
        "\n",
        "import bugan\n",
        "#EXTRACT package version\n",
        "    #switch stdout to temperary stringIO\n",
        "old_stdout = sys.stdout\n",
        "temp_stdout = io.StringIO()\n",
        "sys.stdout = temp_stdout\n",
        "    #get version\n",
        "%pip freeze | grep bugan\n",
        "version = temp_stdout.getvalue()\n",
        "rev_number = version.split(\"+g\")[1].rstrip()\n",
        "    #switch back stdout\n",
        "sys.stdout = old_stdout\n",
        "\n",
        "output.clear()\n",
        "print(\"bugan package revision number: \" + str(rev_number))\n",
        "print('ok!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU8OYjiWtzeo"
      },
      "source": [
        "import io\n",
        "from io import BytesIO\n",
        "import zipfile\n",
        "import trimesh\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "\n",
        "from bugan.functionsPL import *\n",
        "from bugan.modelsPL import VAEGAN, GAN, VAE_train\n",
        "\n",
        "# Ignore excessive warnings\n",
        "import logging\n",
        "logging.propagate = False \n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "from xvfbwrapper import Xvfb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ04363JY9Yt"
      },
      "source": [
        "#load model (either VAEGAN / VAE / GAN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5H_WnkNY_aE"
      },
      "source": [
        "if id:\n",
        "    checkpoint_path = os.path.join(wandb.run.dir, 'checkpoint.ckpt')\n",
        "    load_checkpoint_from_cloud(checkpoint_path = 'checkpoint.ckpt')\n",
        "else:\n",
        "    checkpoint_path = ckpt_file_location\n",
        "\n",
        "#model\n",
        "if selected_model == \"VAEGAN\":\n",
        "    MODEL_CLASS = VAEGAN\n",
        "elif selected_model == \"GAN\":\n",
        "    MODEL_CLASS = GAN\n",
        "else:\n",
        "    MODEL_CLASS = VAE_train\n",
        "    \n",
        "model = MODEL_CLASS.load_from_checkpoint(checkpoint_path)\n",
        "model = model.eval().to(device)\n",
        "\n",
        "#obtain latent vector and store generated mesh files\n",
        "mesh_file = []\n",
        "latent_array = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWG0aa-pcuU8"
      },
      "source": [
        "#helper function\n",
        "\n",
        "#for VAEGAN / VAE\n",
        "def reconstruct_file(vae, f_loc):\n",
        "    f = Path(f_loc)\n",
        "    m = trimesh.load(f, force=\"mesh\")\n",
        "    m = mesh2arrayCentered(m, array_length=32)\n",
        "    m = m[np.newaxis,np.newaxis,:,:,:]\n",
        "    m = torch.Tensor(m).float().type_as(vae.vae_encoder.dis_fc1[0].weight)\n",
        "    m_rec, z, _ = vae(m, output_all=True)\n",
        "\n",
        "    m_rec = m_rec.detach().cpu().numpy()\n",
        "    m_rec = m_rec[0,0,:,:,:]\n",
        "    m_rec_bool_array = m_rec > 0\n",
        "    voxelmesh = netarray2mesh(m_rec_bool_array)\n",
        "    return voxelmesh, z[0].detach().cpu().numpy()\n",
        "\n",
        "#for GAN\n",
        "def create_mesh_random(generator):\n",
        "    z = torch.randn(1, generator.z_size).type_as(generator.gen_fc.weight)\n",
        "    generated_tree = generator(z)[0, 0, :, :, :]\n",
        "    generated_tree = generated_tree.detach().cpu().numpy()\n",
        "\n",
        "    mesh_bool_array = generated_tree > 0\n",
        "    voxelmesh = netarray2mesh(mesh_bool_array)\n",
        "    return voxelmesh, z[0].detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPuEGujJirJl"
      },
      "source": [
        "#latent space walk (for VAE/VAEGAN)\n",
        "$Requirement:$\n",
        "\n",
        "The code below requires **2 meshfiles** to generate 2 latent vectors [$z_1$, $z_2$].\n",
        "\n",
        "$Description:$\n",
        "\n",
        "The VAE in the model has 2 components: encoder / decoder.\n",
        "\n",
        "To generate 2 latent vectors [$z_1$, $z_2$]:\n",
        "\n",
        "$3D mesh 1 → encoder → latent \\ space \\ vector \\ z1$\n",
        "\n",
        "$3D mesh 2 → encoder → latent \\ space \\ vector \\ z2$\n",
        "\n",
        "Then, a line between $z_1$ and $z_2$ will be drawn, and latent space vectors $Z_0, Z_1, ..., Z_{n-1}$ in between will be sampled evenly accroding to the test_num_samples ($n$) set above.\n",
        "\n",
        "$n = test\\_num\\_samples, i \\in [0, n-1]$\n",
        "\n",
        "$Z_i = \\frac{i}{n-1} z_1 + \\frac{n-1-i}{n-1} z_2$ (Note that $Z_0 = z_1$ and $Z_{n-1} = z_2$)\n",
        "\n",
        "Finally, new meshs will be produced based on those latent space vectors $Z_0, Z_1, ..., Z_{n-1}$:\n",
        "\n",
        "$latent \\ space \\ vector \\ Z_i → decoder → generated \\ mesh \\ m_i$\n",
        "\n",
        "All meshes $m_0, m_1, ..., m_i$ will be stored in the export_location set above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taH1etQDiv8z"
      },
      "source": [
        "if selected_model == \"VAEGAN\" or selected_model == \"VAE\":\n",
        "    vae = model.vae\n",
        "\n",
        "    file_loc = [meshfile1, meshfile2]\n",
        "    for f_loc in file_loc:\n",
        "        _, z = reconstruct_file(vae, f_loc)\n",
        "        latent_array.append(z)\n",
        "        \n",
        "    #latent space walk\n",
        "    latent_vectors = np.linspace(latent_array[0], latent_array[1], num=test_num_samples)\n",
        "    # latent_vectors = np.random.randn(test_num_samples,128)\n",
        "    # print(latent_vectors.shape)\n",
        "\n",
        "    #genereate meshes from the latent_vectors\n",
        "    latent_vectors = torch.Tensor(latent_vectors).float().type_as(vae.vae_encoder.dis_fc1[0].weight)\n",
        "    meshes = vae.vae_decoder(latent_vectors)[:, 0, :, :, :]\n",
        "    meshes = meshes.detach().cpu().numpy()\n",
        "\n",
        "    #create export directory\n",
        "    export_location = Path(export_location)\n",
        "    print(export_location.exists())\n",
        "    if not export_location.exists():\n",
        "        export_location.mkdir(parents=True)\n",
        "\n",
        "    #store generated meshes to export_location\n",
        "    for n in range(test_num_samples):\n",
        "        sample_tree_array = meshes[n] > 0\n",
        "        voxelmesh = netarray2mesh(sample_tree_array)\n",
        "        mesh_file.append(voxelmesh)\n",
        "        export_path = export_location / f\"sample_{n}.obj\"\n",
        "        voxelmesh.export(file_obj=export_path, file_type=\"obj\")\n",
        "    print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiLip_2Ieazj"
      },
      "source": [
        "# #visualize\n",
        "# voxelmesh = mesh_file[0]\n",
        "# #rotate mesh a bit so the color/texture is displayed correctly\n",
        "# voxelmesh = voxelmesh.apply_transform(trimesh.transformations.rotation_matrix(1e-8, (0,1,0)))\n",
        "# voxelmesh.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMGw0cvmeu4y"
      },
      "source": [
        "#latent space walk (for GAN)\n",
        "$Requirement:$\n",
        "\n",
        "The code below requires user to select 2 latent vectors [$z_1$, $z_2$] by manually running the 2 select latent vector cells below (One cell for $z_1$ and one cell for $z_2$). \n",
        "\n",
        "Everytime the cell for $z_1$ runs, a new latent vector $z_1$ is generated and replaced the stored latent vector value. The user can check the appearance of the generated mesh from the latent vector $z_1$ to decide whether to keep $z_1$ (same logic for the $z_2$).\n",
        "\n",
        "Please rerun the cell for $z_1$ and $z_2$ until 2 nice-looking meshes appears.\n",
        "\n",
        "$Description:$\n",
        "\n",
        "The GAN mainly use the component generator to generate meshes.\n",
        "\n",
        "To choose 2 latent vectors [$z_1$, $z_2$]:\n",
        "\n",
        "$random \\ generator → latent \\ space \\ vector \\ $z_1$ → generator → generated \\ mesh \\ m_1$\n",
        "\n",
        "$random \\ generator → latent \\ space \\ vector \\ $z_2$ → generator → generated \\ mesh \\ m_2$\n",
        "\n",
        "Then, a line between $z_1$ and $z_2$ will be drawn, and latent space vectors $Z_0, Z_1, ..., Z_{n-1}$ in between will be sampled evenly accroding to the test_num_samples ($n$) set above.\n",
        "\n",
        "$n = test\\_num\\_samples, i \\in [0, n-1]$\n",
        "\n",
        "$Z_i = \\frac{i}{n-1} z_1 + \\frac{n-1-i}{n-1} z_2$ (Note that $Z_0 = z_1$ and $Z_{n-1} = z_2$)\n",
        "\n",
        "Finally, new meshs will be produced based on those latent space vectors $Z_0, Z_1, ..., Z_{n-1}$:\n",
        "\n",
        "$latent \\ space \\ vector \\ Z_i → generator → generated \\ mesh \\ m_i$\n",
        "\n",
        "All meshes $m_0, m_1, ..., m_i$ will be stored in the export_location set above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4VZ5nAMe1HJ"
      },
      "source": [
        "if selected_model == \"GAN\":\n",
        "    generator = model.generator\n",
        "    latent_array = np.zeros((2, generator.z_size))\n",
        "else:\n",
        "    #terminate script here if selected_model is not GAN. (VAEGAN/GAN part located above)\n",
        "    raise Exception(\"selected_model is not GAN. Terminate here!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eDLoYrEhK_E"
      },
      "source": [
        "#select latent vector 1\n",
        "#please keep rerun this cell until \"good\" mesh appears\n",
        "voxelmesh = None\n",
        "voxelmesh, z = create_mesh_random(generator)\n",
        "latent_array[0] = z\n",
        "\n",
        "#rotate mesh a bit so the color/texture is displayed correctly\n",
        "voxelmesh = voxelmesh.apply_transform(trimesh.transformations.rotation_matrix(1e-8, (0,1,0)))\n",
        "# mesh.show() does not work within if statement?\n",
        "voxelmesh.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VD5Jl6ghcds"
      },
      "source": [
        "#select latent vector 2\n",
        "#please keep rerun this cell until \"good\" mesh appears\n",
        "voxelmesh = None\n",
        "voxelmesh, z = create_mesh_random(generator)\n",
        "latent_array[1] = z\n",
        "\n",
        "#rotate mesh a bit so the color/texture is displayed correctly\n",
        "voxelmesh = voxelmesh.apply_transform(trimesh.transformations.rotation_matrix(1e-8, (0,1,0)))\n",
        "# mesh.show() does not work within if statement?\n",
        "voxelmesh.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBOSt9TAhdx6"
      },
      "source": [
        "#latent space walk\n",
        "latent_vectors = np.linspace(latent_array[0], latent_array[1], num=test_num_samples)\n",
        "# latent_vectors = np.random.randn(test_num_samples,128)\n",
        "# print(latent_vectors.shape)\n",
        "\n",
        "#genereate meshes from the latent_vectors\n",
        "latent_vectors = torch.Tensor(latent_vectors).float().type_as(generator.gen_fc.weight)\n",
        "meshes = generator(latent_vectors)[:, 0, :, :, :]\n",
        "meshes = meshes.detach().cpu().numpy()\n",
        "\n",
        "#create directory\n",
        "export_location = Path(export_location)\n",
        "print(export_location.exists())\n",
        "if not export_location.exists():\n",
        "    export_location.mkdir(parents=True)\n",
        "\n",
        "for n in range(test_num_samples):\n",
        "    sample_tree_array = meshes[n] > 0\n",
        "    voxelmesh = netarray2mesh(sample_tree_array)\n",
        "    mesh_file.append(voxelmesh)\n",
        "    export_path = export_location / f\"sample_{n}.obj\"\n",
        "    voxelmesh.export(file_obj=export_path, file_type=\"obj\")\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6EJlLkHiCmM"
      },
      "source": [
        "# #visualize\n",
        "# voxelmesh = mesh_file[0]\n",
        "# #rotate mesh a bit so the color/texture is displayed correctly\n",
        "# voxelmesh = voxelmesh.apply_transform(trimesh.transformations.rotation_matrix(1e-8, (0,1,0)))\n",
        "# voxelmesh.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}