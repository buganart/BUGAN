{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "script_GAN_testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/BUGAN/blob/master/script_GAN_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQtEpYYRh9LM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c44b42bf-165e-45f2-f968-aeb0e1c37b42"
      },
      "source": [
        "#mount google drive\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3jIEKP7i4Nt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3332544-3953-49ca-ceae-165a77b32a99"
      },
      "source": [
        "#right click shared folder IRCMS_GAN_collaborative_database and \"Add shortcut to Drive\" to My drive\n",
        "%cd drive/My Drive/IRCMS_GAN_collaborative_database/\n",
        "\n",
        "#record paths to resources\n",
        "data_path = \"Research/Peter/Tree_3D_models_obj/obj_files/\"\n",
        "run_path = \"Experiments/colab-treegan/\"\n",
        "\n",
        "# !ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1ylB2p6N0qQ-G4OsBuwcZ9C0tsqVu9ww4/IRCMS_GAN_collaborative_database\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LzAiBuWu6pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install dependencies - OSMesa, mmatl's fork of PyOpenGL\n",
        "!sudo apt update\n",
        "!sudo wget https://github.com/mmatl/travis_debs/raw/master/xenial/mesa_18.3.3-0.deb\n",
        "!sudo dpkg -i ./mesa_18.3.3-0.deb || true\n",
        "!sudo apt install -f\n",
        "!git clone https://github.com/mmatl/pyopengl.git\n",
        "!pip install ./pyopengl\n",
        "\n",
        "!pip install trimesh\n",
        "# !pip install meshrender==0.0.1\n",
        "# !pip install autolab_perception\n",
        "!pip install wandb -q\n",
        "output.clear()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYpnFurJt0Pu",
        "colab_type": "text"
      },
      "source": [
        "#add libraries, and login to wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU8OYjiWtzeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import os\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
        "import trimesh\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "# Ignore excessive warnings\n",
        "import logging\n",
        "logging.propagate = False \n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "# WandB â€“ Import the wandb library\n",
        "import wandb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_fQzoLaVP2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wandb login\n",
        "output.clear()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2lZ9OiNchg0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "998322f0-6cc8-4ac8-f948-d72a4033e896"
      },
      "source": [
        "#id None to start a new run. For resuming run, put the id of the run below\n",
        "id = \"r968ai0b\"\n",
        "resume = False\n",
        "if id is None:\n",
        "    id = wandb.util.generate_id()\n",
        "else:\n",
        "    resume = True\n",
        "\n",
        "run = wandb.init(project=\"tree-gan\", id=id, resume=\"allow\", dir=run_path)\n",
        "print(\"run id:\" + str(wandb.run.id))\n",
        "wandb.run.name = str(wandb.run.id)\n",
        "wandb.watch_called = False\n",
        "wandb.run.save_code = False"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/bugan/tree-gan\" target=\"_blank\">https://app.wandb.ai/bugan/tree-gan</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/bugan/tree-gan/runs/r968ai0b\" target=\"_blank\">https://app.wandb.ai/bugan/tree-gan/runs/r968ai0b</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "run id:r968ai0b\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Streaming file created twice in same run: Experiments/colab-treegan/wandb/run-20200812_132707-r968ai0b/wandb-history.jsonl\n",
            "Streaming file created twice in same run: Experiments/colab-treegan/wandb/run-20200812_132707-r968ai0b/wandb-events.jsonl\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww3zbHkxAw8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#keep track of hyperparams\n",
        "config = wandb.config\n",
        "config.batch_size = 4\n",
        "config.epochs = 1000\n",
        "config.g_lr = 0.0001\n",
        "config.g_layer = 2\n",
        "config.d_lr = 0.00003           \n",
        "config.d_layer = 1\n",
        "config.seed = 1234\n",
        "config.log_interval = 20\n",
        "config.data_augmentation = True"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91yyJjoz4S4q",
        "colab_type": "text"
      },
      "source": [
        "#model description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYRWtkMI4SoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input: 128-d noise vector\n",
        "#output: (250,250,250) array with values in [0,1]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.fc_channel = 8 #16\n",
        "        self.fc_size = 4\n",
        "\n",
        "        num_unit1 = self.fc_channel   \n",
        "        num_unit2 = 32   #32\n",
        "        num_unit3 = 32   #64\n",
        "        num_unit4 = 64   #128\n",
        "        num_unit5 = 128   #256\n",
        "        num_unit6 = 128   #512\n",
        "        self.gen_fc = nn.Linear(128, num_unit1 * self.fc_size * self.fc_size * self.fc_size)\n",
        "        self.gen = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose3d(num_unit1, num_unit2, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit2, num_unit3, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit3),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit3, num_unit3, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit3),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit3, num_unit3, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit3),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='trilinear'),\n",
        "\n",
        "\n",
        "            nn.ConvTranspose3d(num_unit3, num_unit4, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit4, num_unit4, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit4, num_unit4, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit4, num_unit4, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='trilinear'),\n",
        "\n",
        "\n",
        "            nn.ConvTranspose3d(num_unit4, num_unit5, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit5),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit5, num_unit5, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit5),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit5, num_unit6, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit6),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit6, num_unit6, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit6),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='trilinear'),\n",
        "\n",
        "            nn.ConvTranspose3d(num_unit6, num_unit6, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit6),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit6, num_unit6, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit6),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit6, num_unit5, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit5),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit5, num_unit5, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit5),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='trilinear'),\n",
        "\n",
        "            nn.ConvTranspose3d(num_unit5, num_unit4, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit4, num_unit3, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit3),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit3, num_unit2, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit2, num_unit1, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose3d(num_unit1, 1, 3, 1, padding = 1),\n",
        "            nn.Sigmoid()                #tanh: voxel representation [-1,1], sigmoid: [0,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.gen_fc(x)\n",
        "        x = x.view(x.shape[0], self.fc_channel, self.fc_size, self.fc_size, self.fc_size)\n",
        "        x = self.gen(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        num_unit1 = 8   #16\n",
        "        num_unit2 = 16   #32\n",
        "        num_unit3 = 32   #64\n",
        "        num_unit4 = 32  #128\n",
        "        num_unit5 = 64   #256\n",
        "        num_unit6 = 64   #512\n",
        "        \n",
        "        self.dis = nn.Sequential(\n",
        "            nn.Conv3d(1, num_unit1, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv3d(num_unit1, num_unit2, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv3d(num_unit2, num_unit3, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit3),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.MaxPool3d((2, 2, 2)),\n",
        "\n",
        "            nn.Conv3d(num_unit3, num_unit3, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv3d(num_unit3, num_unit4, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.MaxPool3d((2, 2, 2)),\n",
        "\n",
        "            nn.Conv3d(num_unit4, num_unit5, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv3d(num_unit5, num_unit5, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv3d(num_unit5, num_unit4, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.MaxPool3d((2, 2, 2)),\n",
        "\n",
        "            nn.Conv3d(num_unit4, num_unit3, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit3),\n",
        "            nn.ReLU(True),            \n",
        "            nn.Conv3d(num_unit3, num_unit2, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.MaxPool3d((2, 2, 2)),\n",
        "\n",
        "\n",
        "            nn.Conv3d(num_unit2, num_unit1, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv3d(num_unit1, num_unit1, 3, 1, padding = 1),\n",
        "            nn.BatchNorm3d(num_unit1),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        self.dis_fc1 = nn.Sequential(\n",
        "            nn.Linear(num_unit1 * 4 * 4 * 4, 128),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.dis_fc2 = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.dis(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        fx = self.dis_fc1(x)\n",
        "        x = self.dis_fc2(fx)\n",
        "        return x, fx\n",
        "\n",
        "\n",
        "class GAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GAN, self).__init__()\n",
        "        self.generator = Generator()\n",
        "        self.discriminator = Discriminator()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.generator(x)\n",
        "        x = self.discriminator(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbrRSqnqK20W",
        "colab_type": "text"
      },
      "source": [
        "#functions for pytorch network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHZ9sGFVspXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def netarray2indices(array):\n",
        "    coord_list = []\n",
        "    if len(array.shape) == 5:\n",
        "        array = array[0][0]\n",
        "    x,y,z = array.shape\n",
        "    for i in range(x):\n",
        "        for j in range(y):\n",
        "            for k in range(z):\n",
        "                if array[i,j,k] > 0.5:        #tanh: voxel representation [-1,1], sigmoid: [0,1]\n",
        "                    coord_list.append([i,j,k])\n",
        "    print(len(coord_list))\n",
        "    if len(coord_list) == 0:\n",
        "        return np.array([[0,0,0]])  #return at least one point to prevent wandb 3dobject error\n",
        "    return np.array(coord_list)\n",
        "\n",
        "# array should be 3d\n",
        "def netarray2mesh(array, returnWandb=True):\n",
        "    if len(array.shape) != 3:\n",
        "        raise Exception(\"netarray2mesh: input array should be 3d\")\n",
        "\n",
        "    #convert to bool dtype\n",
        "    array = array > 0.5\n",
        "    #array all zero gives error\n",
        "    if np.sum(array) == 0:\n",
        "        array[0,0,0] = True\n",
        "    voxelmesh = trimesh.voxel.base.VoxelGrid(trimesh.voxel.encoding.DenseEncoding(array)).marching_cubes\n",
        "\n",
        "    if returnWandb:\n",
        "        voxelmeshfile = voxelmesh.export(file_type='obj')\n",
        "        voxelmeshfile = wandb.Object3D(io.StringIO(voxelmeshfile),file_type='obj')\n",
        "\n",
        "        return voxelmesh, voxelmeshfile\n",
        "    else:\n",
        "        return voxelmesh, _\n",
        "\n",
        "\n",
        "def save_model(model, model_path = os.path.join(wandb.run.dir, 'model_dict.pth')):\n",
        "\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    wandb.save(model_path)\n",
        "\n",
        "def load_model(model_path = 'model_dict.pth'):\n",
        "    model = GAN()\n",
        "    model_file = wandb.restore(model_path)\n",
        "    model.load_state_dict(torch.load(model_file.name))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def generate_tree(model, num_trees = 1, num_try = 3):\n",
        "    #num_try is number of trial to generate a tree that can fool D\n",
        "    #total number of sample generated = num_trees * num_try\n",
        "    \n",
        "\n",
        "    model.to(device).eval()\n",
        "    generator = model.generator.to(device).eval()\n",
        "    discriminator = model.discriminator.to(device).eval()\n",
        "\n",
        "    result = None\n",
        "\n",
        "\n",
        "    if num_try <= 0:\n",
        "        num_tree_total = num_trees\n",
        "        num_runs = int(np.ceil(num_tree_total / config.batch_size))\n",
        "        #ignore discriminator\n",
        "        for i in range(num_runs):\n",
        "            #generate noise vector\n",
        "            z = torch.randn(config.batch_size, 128).to(device)\n",
        "            \n",
        "            tree_fake = generator(z)[:,0,:,:,:]\n",
        "            selected_trees = tree_fake.detach().cpu().numpy()\n",
        "            if result is None:\n",
        "                result = selected_trees\n",
        "            else:\n",
        "                result = np.concatenate((result, selected_trees), axis=0)\n",
        "    else:\n",
        "        num_tree_total = num_trees * num_try\n",
        "        num_runs = int(np.ceil(num_tree_total / config.batch_size))\n",
        "        #only show samples can fool discriminator\n",
        "        for i in range(num_runs):\n",
        "            #generate noise vector\n",
        "            z = torch.randn(config.batch_size, 128).to(device)\n",
        "            \n",
        "            tree_fake = generator(z)\n",
        "            dout, _ = discriminator(tree_fake)\n",
        "            dout = dout > 0.5\n",
        "            selected_trees = tree_fake[dout].detach().cpu().numpy()\n",
        "            if result is None:\n",
        "                result = selected_trees\n",
        "            else:\n",
        "                result = np.concatenate((result, selected_trees), axis=0)\n",
        "    #select at most num_trees\n",
        "    if result.shape[0] > num_trees:\n",
        "        result = result[:num_trees]\n",
        "    #in case no good result\n",
        "    if result.shape[0] <= 0:\n",
        "        result = np.zeros((1,64,64,64))\n",
        "        result[0,0,0,0] = 1\n",
        "    return result\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1TU9N4Y8se3",
        "colab_type": "text"
      },
      "source": [
        "#show generated trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFQWIWapYOsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8335fb7c-f19f-4ff1-efef-6b853539a31e"
      },
      "source": [
        "model = load_model()\n",
        "\n",
        "\n",
        "#set seed\n",
        "torch.manual_seed(config.seed)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f6c991df9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYn7mPBhmYav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "ad85e1f9-187a-4e94-8dd9-8e2b3de94bc9"
      },
      "source": [
        "sample_tree_array = generate_tree(model, num_trees=10, num_try=0)\n",
        "print(sample_tree_array.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(10, 64, 64, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L-7F9SBmlDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mesh_list = []\n",
        "for i in range(sample_tree_array.shape[0]):\n",
        "    array = sample_tree_array[i]\n",
        "    mesh,_ = netarray2mesh(array, returnWandb=False)\n",
        "    mesh_list.append(mesh)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiD3B52wxyiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mesh1 = mesh_list[0]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzfmdPiKTYSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import meshrender\n",
        "# from perception import CameraIntrinsics\n",
        "# scene = meshrender.Scene()\n",
        "# mesh_obj = meshrender.SceneObject(mesh_list[0])\n",
        "# scene.add_object('mesh', mesh_obj)\n",
        "\n",
        "# dir1 = meshrender.DirectionalLight((0,0,1), color=(1,1,0), strength=2)\n",
        "# dir2 = meshrender.DirectionalLight((0,0,-1), color=(1,0,0), strength=1)\n",
        "# scene.add_light('dir_light1', dir1)\n",
        "# scene.add_light('dir_light2', dir2)\n",
        "\n",
        "# ci = CameraIntrinsics(\n",
        "#     frame = 'camera',\n",
        "#     fx = 525.0,\n",
        "#     fy = 525.0,\n",
        "#     cx = 319.5,\n",
        "#     cy = 239.5,\n",
        "#     skew=0.0,\n",
        "#     height=480,\n",
        "#     width=640\n",
        "# )\n",
        "\n",
        "# camera = meshrender.VirtualCamera(ci)\n",
        "# scene.camera = camera\n",
        "\n",
        "# color_image_raw, depth_image_raw = scene.render(render_color=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJgeDpI_089U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "a9401955-5c57-4152-e29d-4df4f4cb949c"
      },
      "source": [
        "!pip install pyrender\n",
        "!git clone https://github.com/mmatl/pyrender.git"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyrender in /usr/local/lib/python3.6/dist-packages (0.1.43)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from pyrender) (2.4.1)\n",
            "Requirement already satisfied: pyglet>=1.4.10 in /usr/local/lib/python3.6/dist-packages (from pyrender) (1.5.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pyrender) (1.15.0)\n",
            "Requirement already satisfied: freetype-py in /usr/local/lib/python3.6/dist-packages (from pyrender) (2.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from pyrender) (2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyrender) (1.4.1)\n",
            "Requirement already satisfied: PyOpenGL==3.1.0 in /usr/local/lib/python3.6/dist-packages (from pyrender) (3.1.0)\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.6/dist-packages (from pyrender) (3.7.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyrender) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from pyrender) (7.0.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->pyrender) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from trimesh->pyrender) (49.2.0)\n",
            "fatal: destination path 'pyrender' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxaTzx5xxS9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "8e91fe43-6cbd-4f4f-868c-66effe8512ac"
      },
      "source": [
        "import os\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
        "import numpy as np\n",
        "import trimesh\n",
        "import pyrender\n",
        "\n",
        "# Load the FUZE bottle trimesh and put it in a scene\n",
        "mesh = pyrender.Mesh.from_trimesh(mesh1)\n",
        "scene = pyrender.Scene()\n",
        "scene.add(mesh)\n",
        "\n",
        "# Set up the camera -- z-axis away from the scene, x-axis right, y-axis up\n",
        "camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0)\n",
        "s = np.sqrt(2)/2\n",
        "camera_pose = trimesh.transformations.translation_matrix((50,50, 100))\n",
        "scene.add(camera, pose=camera_pose)\n",
        "\n",
        "# Set up the light -- a single spot light in the same spot as the camera\n",
        "light = pyrender.SpotLight(color=[0,255,0], intensity=1.0,\n",
        "                               innerConeAngle=np.pi/16.0)\n",
        "scene.add(light, pose=camera_pose)\n",
        "\n",
        "# Render the scene\n",
        "r = pyrender.OffscreenRenderer(1080, 960)\n",
        "color, depth = r.render(scene)\n",
        "\n",
        "# Show the images\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis('off')\n",
        "plt.imshow(color)\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.axis('off')\n",
        "# plt.imshow(depth, cmap=plt.cm.gray_r)\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}