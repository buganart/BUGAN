{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom dataset of script_VAEGAN_voxelsize1_lightning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/BUGAN/blob/master/custom_dataset_of_script_VAEGAN_voxelsize1_lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQtEpYYRh9LM"
      },
      "source": [
        "#mount google drive\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if we have linked the folder\n",
        "from pathlib import Path\n",
        "if not Path(\"/content/drive/My Drive/IRCMS_GAN_collaborative_database\").exists():\n",
        "    print(\n",
        "        \"Shortcut to our shared drive folder doesn't exits.\\n\\n\"\n",
        "        \"\\t1. Go to the google drive web UI\\n\"\n",
        "        \"\\t2. Right click shared folder IRCMS_GAN_collaborative_database and click \\\"Add shortcut to Drive\\\"\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp1sqgZf5lRv"
      },
      "source": [
        "!pip install wandb==0.9.7\n",
        "output.clear()\n",
        "import wandb\n",
        "!wandb login\n",
        "output.clear()\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b-Vx_QVHLXH",
        "cellView": "both"
      },
      "source": [
        "#@title Dataset\n",
        "#@markdown - set `None`if you want to start a new run\n",
        "#@markdown - set `\"run_id\"` if you want to resume a run (for example: `u9imsvva`)\n",
        "#@markdown - The id of the current run is shown below in the cell with `wandb.init()` or you can find it in the wandb web UI.\n",
        "id = None #@param {type:\"string\"}\n",
        "#@markdown Enter project name (either `handtool-gan` or `tree-gan`)\n",
        "project_name = \"tree-gan\" #@param [\"tree-gan\", \"handtool-gan\"]\n",
        "#@markdown Enter file location.  \n",
        "#@markdown - For example via the file browser on the left to locate and right click to copy the path.)\n",
        "#@markdown - zipfile example: `/content/drive/My Drive/h/k/a.zip`\n",
        "#@markdown - file folder example: `/content/drive/My Drive/h/k` \n",
        "file_location = \"/content/drive/My Drive/IRCMS_GAN_collaborative_database/Research/Peter/Chairs_Princeton/chair_train.zip\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"id:\", id)\n",
        "print(\"project_name:\", project_name)\n",
        "print(\"file_location:\", file_location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUduMFlzmKmO"
      },
      "source": [
        "# To just train a model, no edits should be required in any cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3jIEKP7i4Nt"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "%cd /content/drive/My Drive/IRCMS_GAN_collaborative_database/Experiments/\n",
        "if project_name == \"tree-gan\":\n",
        "    %cd colab-treegan/\n",
        "else:\n",
        "    %cd colab-handtool/\n",
        "\n",
        "dataset_path = Path(file_location)\n",
        "run_path = \"./\"\n",
        "\n",
        "if file_location.endswith(\".zip\"):\n",
        "    dataset_name = dataset_path.stem\n",
        "else:\n",
        "    dataset_name = \"dataset_array_custom\"\n",
        "\n",
        "!apt-get update\n",
        "\n",
        "!pip install pytorch-lightning==0.9.0\n",
        "!pip install trimesh\n",
        "!apt install -y xvfb\n",
        "!pip install trimesh xvfbwrapper\n",
        "output.clear()\n",
        "print('ok!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU8OYjiWtzeo"
      },
      "source": [
        "import io\n",
        "from io import BytesIO\n",
        "import zipfile\n",
        "import trimesh\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# Ignore excessive warnings\n",
        "import logging\n",
        "logging.propagate = False \n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "from xvfbwrapper import Xvfb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2lZ9OiNchg0"
      },
      "source": [
        "resume = False\n",
        "if id is None:\n",
        "    id = wandb.util.generate_id()\n",
        "else:\n",
        "    resume = True\n",
        "\n",
        "run = wandb.init(project=project_name, id=id, entity=\"bugan\", resume=True, dir=run_path)\n",
        "print(\"run id: \" + str(wandb.run.id))\n",
        "print(\"run name: \" + str(wandb.run.name))\n",
        "wandb.watch_called = False\n",
        "wandb.run.save_code = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww3zbHkxAw8B"
      },
      "source": [
        "#keep track of hyperparams\n",
        "config = wandb.config\n",
        "\n",
        "config.batch_size = 8\n",
        "config.array_size = 32\n",
        "\n",
        "config.z_size = 128\n",
        "config.gen_num_layer_unit = [1024, 512, 256, 128]\n",
        "config.dis_num_layer_unit = [32, 64, 128, 128]\n",
        "config.leakyReLU = False    #leakyReLU implementation still not in modelPL\n",
        "config.balance_voxel_in_space = False\n",
        "\n",
        "config.epochs = 3000\n",
        "config.vae_lr = 0.0025\n",
        "config.vae_encoder_layer = 1\n",
        "config.vae_decoder_layer = 2\n",
        "config.d_lr = 0.00005            \n",
        "config.d_layer = 1\n",
        "config.vae_recon_loss_factor = 1\n",
        "config.seed = 1234\n",
        "config.log_image_interval = 5\n",
        "config.log_mesh_interval = 50\n",
        "config.data_augmentation = True\n",
        "config.num_augment_data = 4\n",
        "\n",
        "config.vae_opt = \"Adam\"\n",
        "config.dis_opt = \"Adam\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alz0eunxK9hI"
      },
      "source": [
        "#dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whLlZwwAoKuR"
      },
      "source": [
        "### load our package\n",
        "\n",
        "#clone then install\n",
        "# !git clone https://github.com/buganart/BUGAN repo\n",
        "# !pip install -e ./repo/\n",
        "# import site\n",
        "# site.main()\n",
        "\n",
        "#directly install using pip\n",
        "!pip install -U git+https://github.com/buganart/BUGAN.git#egg=bugan\n",
        "\n",
        "from bugan.functionsPL import *\n",
        "from bugan.modelsPL import VAEGAN, VAE, Discriminator, Generator\n",
        "\n",
        "# from functionsPL import *\n",
        "# from modelsPL import VAEGAN, VAE, Discriminator, Generator\n",
        "\n",
        "run.tags.append(\"VAEGAN\")\n",
        "run.group = \"VAEGAN\"\n",
        "\n",
        "###     load dataset\n",
        "np.random.seed(config.seed)\n",
        "# dataModule = DataModule(config, run)\n",
        "# config.num_data = dataModule.size\n",
        "\n",
        "config.dataset = dataset_name\n",
        "dataModule = DataModule_process(config, run, dataset_path)\n",
        "config.num_data = dataModule.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1TU9N4Y8se3"
      },
      "source": [
        "#train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vzV_Mzfd_MZ"
      },
      "source": [
        "#set seed\n",
        "torch.manual_seed(config.seed)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "#render setup\n",
        "vdisplay = Xvfb()\n",
        "vdisplay.start()\n",
        "\n",
        "#wandb logger setup\n",
        "wandb_logger = WandbLogger(experiment=run, log_model=True)\n",
        "\n",
        "checkpoint_path = os.path.join(wandb.run.dir, 'checkpoint.ckpt')\n",
        "\n",
        "if resume:\n",
        "    #get file from the wandb cloud\n",
        "    load_checkpoint_from_cloud(checkpoint_path = 'checkpoint.ckpt')\n",
        "    #restore training state completely\n",
        "    trainer = pl.Trainer(max_epochs = config.epochs, logger=wandb_logger, checkpoint_callback = None, resume_from_checkpoint = checkpoint_path)\n",
        "else:\n",
        "    trainer = pl.Trainer(max_epochs = config.epochs, logger=wandb_logger, checkpoint_callback = None)\n",
        "\n",
        "#model\n",
        "vaegan = VAEGAN(config, trainer, save_model_path = checkpoint_path).to(device)\n",
        "wandb_logger.watch(vaegan)\n",
        "\n",
        "trainer.fit(vaegan, dataModule)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}